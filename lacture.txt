from pyspark.sql import SparkSession
from pyspark.sql.functions import pandas_udf, PandasUDFType
import pandas as pd
import numpy as np
import joblib

# Create a Spark session
spark = SparkSession.builder.appName("ScikitToPyspark").getOrCreate()

# Path to the saved scikit-learn model
scikit_model_path = 'path_to_saved_model.pkl'

# Load scikit-learn model
scikit_rf_model = joblib.load(scikit_model_path)

# Load your data into a PySpark DataFrame (assuming you have a DataFrame named 'data')
# Replace with your data loading approach
# data = spark.read.csv('path_to_your_data.csv', header=True, inferSchema=True)

# Define the feature columns
feature_columns = ['col1', 'col2', 'col3', 'col4']  # Replace with your feature column names

# Define a Pandas UDF to apply predictions on each partition
@pandas_udf('int', PandasUDFType.SCALAR)
def predict_udf(pdf):
    local_model = joblib.load(scikit_model_path)
    features = pdf[feature_columns].values
    predictions = local_model.predict(features)
    return pd.Series(predictions)

# Apply predictions using the defined UDF
result_df = data.withColumn("prediction", predict_udf("*"))

# Show or further process the result_df DataFrame
result_df.show()

# Stop the Spark session when finished
spark.stop()
