from pyspark.sql import SparkSession
import numpy as np
import joblib

# Create a Spark session
spark = SparkSession.builder.appName("ScikitToPyspark").getOrCreate()

# Path to the saved scikit-learn model
scikit_model_path = 'path_to_saved_model.pkl'

# Load scikit-learn model
scikit_rf_model = joblib.load(scikit_model_path)

# Load your data into a PySpark DataFrame (assuming you have a DataFrame named 'data')
# Replace with your data loading approach
# data = spark.read.csv('path_to_your_data.csv', header=True, inferSchema=True)

# Define the feature columns
feature_columns = ['col1', 'col2', 'col3', 'col4']  # Replace with your feature column names

# Function to predict on a partition of data
def predict_partition(rows):
    local_model = joblib.load(scikit_model_path)
    features = np.array([[row[col] for col in feature_columns] for row in rows])
    predictions = local_model.predict(features)
    return predictions.tolist()

# Apply predictions using mapPartitions
predictions = data.repartition(100).rdd.mapPartitions(predict_partition)

# Combine predictions back to a DataFrame
result_df = spark.createDataFrame(data.rdd.zip(predictions).map(lambda x: x[0] + (x[1],)), schema=data.schema.add("prediction", "int"))

# Show or further process the result_df DataFrame
result_df.show()

# Stop the Spark session when finished
spark.stop()
